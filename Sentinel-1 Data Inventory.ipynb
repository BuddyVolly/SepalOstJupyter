{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open SAR Toolkit (OST) - Jupyter Notebook\n",
    "## Sentinel-1 data inventory and download for large-scale time-series/timescan production\n",
    "\n",
    "This notebook helps you to find specific Sentinel-1 data products for a given area and time of interest.\n",
    "\n",
    "**The general idea is a 3-step approach:**\n",
    "- **Step 1:** Data inventory from full catalogue creating a preliminary inventory file of available acquisitions\n",
    "- **Step 2:** Refine the inventory for homogeneous coverage compatible with the further processing logic\n",
    "- **Step 3:** Download the data to your local machine/AWS instance\n",
    "\n",
    "The **output** is either a shapefile, or it can be put into a PostGreSql/PostGIS database (needs to be configured beforehand). The output is fundamental for further steps of data refinement, download and the processing itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (just execute) Add libraries needed to make the notebook work\n",
    "\n",
    "Please execute this cell (Shift + Enter) in order to add the necessary functionality to make the subsequent commands work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some standard python libs \n",
    "import os\n",
    "\n",
    "# Add the Open SAR Toolkit libs \n",
    "from ost.helpers import scihub, vector as vec\n",
    "from ost.s1 import search, refine, download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a project directory and sub-folders, where all of the data and metadata will be organized\n",
    "\n",
    "Each processing should go into a different project folder. Inside a project folder a pre-defined set of subfolders will be created. Within the subfolders, different data products will be stored during the complete workflow (e.g. downloaded products, inventory shapefiles etc...).\n",
    "\n",
    "Make sure that your project folder is located on a disk with enough disk space available. One Sentinel-1 GRD frame is about 1 GB. All data will be downloaded before processing.\n",
    "\n",
    "If you are familiar with python, you can also change the subfolders to different places, but you need to keep track of it for the processing (Sentinel-1 large-scale Time-series/Timescan rocessing notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main project directory\n",
    "prjDir = '/home/avollrath/OSTdemo'\n",
    "\n",
    "#----------------------------------------------\n",
    "# do not edit this part\n",
    "os.makedirs(prjDir, exist_ok=True)\n",
    "\n",
    "# this is where we download the original scenes\n",
    "dwnDir = '{}/download'.format(prjDir)\n",
    "os.makedirs(dwnDir, exist_ok=True)\n",
    "\n",
    "# this folder will be used for the inventory shape files\n",
    "invDir = '{}/inventory'.format(prjDir)\n",
    "os.makedirs(invDir, exist_ok=True)\n",
    "\n",
    "# this folder will be used for the processed data\n",
    "prcDir = '{}/processed'.format(prjDir)\n",
    "os.makedirs(prcDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the search parameters\n",
    "\n",
    "Similar to the Copernicus scihub interface you need to define some of the search parameters.\n",
    "A special feature is that you can search directly for available data of a specific country by using the ISO3 country code. A list of ISO3 country codes can be found here: https://unstats.un.org/unsd/tradekb/knowledgebase/country-code\n",
    "\n",
    "Also note that except for the output parameter, all parameters can be commented out. In this case a wildcard operator is used to disable the filter of this specific search parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Area of interest\n",
    "#----------------------------\n",
    "\n",
    "# Here we can either point to a shapefile or as well use \n",
    "# an ISO3 country code for the use of national boundaries\n",
    "# In case you want to check for the whole globe comment out.\n",
    "\n",
    "#aoi = '/path/to/a/shapefile'     # absolute path to a shapefile \n",
    "aoi = 'ECU'                                               # alternative use of ISO3 countrycode \n",
    "\n",
    "\n",
    "#----------------------------\n",
    "# Time of interest\n",
    "#----------------------------\n",
    "\n",
    "# Here we set the start and end date for the time period of interest. \n",
    "# If you comment both out, the full mission time period up to today will be considered. \n",
    "startDate = '2017-01-01'                              # data format (YYYY-MM-DD)\n",
    "endDate = '2017-12-31'                                # data format (YYYY-MM-DD)\n",
    "\n",
    "#----------------------------\n",
    "# Output file/table\n",
    "#----------------------------\n",
    "\n",
    "# This can either be a shapefile, in which case it needs to end with '.shp' \n",
    "# or point to a existing or non-existing PostGreSql table\n",
    "output = '{}/fullInventory.shp'.format(invDir)         # name of a PostGreSQL table or a shapefile \n",
    "\n",
    "\n",
    "#----------------------------\n",
    "# Product Type Specification \n",
    "#----------------------------\n",
    "\n",
    "# Here we define what kinds of products we are looking for. You can comment out the ones for which you \n",
    "# want to retrieve all types of products.\n",
    "prdType = 'GRD'                                       # RAW, SLC, GRD or * (for all)\n",
    "polarisation = '*'                                    # VV, VH, HH, HV or * (for all)\n",
    "beamMode = 'IW'                                       # IW, EW, SM or * for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (just execute) Trigger the search\n",
    "\n",
    "You **SHOULD NOT** change anything here after. Just execute the cell with Shift+Enter. \n",
    "Your _**Copernicus scihub credentials**_ will be asked and you will need a working internet connection.\n",
    "\n",
    "In case you do not have a scihub account, please go here to register: https://scihub.copernicus.eu/dhus/#/home\n",
    "\n",
    "**PLEASE NOTE** that OST actually queries the Copernicus Apihub (i.e. a different server), for which user credentials will be transfered only after a week of registration to the standard open scihub. So you may need to wait a couple of days after first registration before it works. For more info, go here:\n",
    "https://scihub.copernicus.eu/twiki/do/view/SciHubWebPortal/APIHubDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct the search command (do not change)\n",
    "aoiStr = scihub.createAoiWkt(aoi)\n",
    "toiStr = scihub.createToiStr(startDate, endDate)\n",
    "prodSpecsStr = scihub.createS1ProdSpecs(prdType, polarisation, beamMode)\n",
    "query = scihub.createQuery('Sentinel-1', aoiStr, toiStr, prodSpecsStr)\n",
    "uname, pword = scihub.askScihubCreds()\n",
    "\n",
    "# execute Search\n",
    "search.s1Scihub(query, output, uname, pword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (just execute) Display the initial results\n",
    "\n",
    "Execute this cell and it will tell you how many scenes have been found and display a map with the AOI and footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (13, 13)\n",
    "\n",
    "# re-read output file into a GeoDataFrame for further steps\n",
    "footprintGdf = refine.readS1Inventory(output)\n",
    "\n",
    "print(' INFO: Found {} products'\n",
    "      ' for {}'\n",
    "      ' between {} and {}'.format(len(footprintGdf), aoi, startDate, endDate))\n",
    "\n",
    "vec.plotInv(aoi, footprintGdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (just execute) Search Refinement\n",
    "\n",
    "The results returned by the search algorithm on Copernicus scihub might not be 100% appropriate to what we are looking for. In this step we refine the results adressing possible issues and reduce later processing needs.\n",
    "\n",
    "A first step **splits the data** by **orbit direction** (i.e. ascending and descending) and **polarization mode** (i.e. VV, VV/VH, HH, HH/HV) and checks the coverage for the resulting combinations (e.g. descending VV/VH polarization). If one combination results in a non-full overlap to the AOI, all further steps are disregarded. In case a full coverage is possbile further refinement steps are taken: \n",
    "\n",
    "1. Some of the acquisition frames might have been processed and/or stored **more than once** in the ESA ground segment. Therefore they appear twice, with the scene identifier that only changes for the last 4 digits. It is necessary to identify those scenes in order to avoid redundancy. We therefore take the ones with the latest ingestion date to assure the use of the latest processor. \n",
    "\n",
    "2. Some of the scenes returned by the search query are actually **not overlapping the AOI**. This is because the search algorithm will actually check for data within a square defined by the outer bounds of the AOI geometry and not the AOI itself. The refinement only takes those frames overlapping with the AOI in order to reduce unnecassary processing later on.\n",
    "\n",
    "3. In the case of **ascending tracks that cross the equator**, the **orbit number** of the frames will **increase by 1** even though they are practically from the same acquisition. During processing the frames need to be merged and the relative orbit numbers (i.e. tracks) should be the same. The metadata in the inventory is therefore updated in order to normalize the relative orbit number.\n",
    "\n",
    "4. (optional) The tracks of Sentinel-1 overlap to a certain degree. The data inventory might return tracks that only **marginally cross the AOI**, but there AOI overlap is already covered by the adjacent track. Thus, if tracks do not contribute to the overall overlap of the AOI, they are disregarded.\n",
    "\n",
    "5. (optional) Some acquisitions might **not cross the entire AOI**. For the subsequent time-series/timescan processing this becomes problematic, since the generation of the time-series will only consider the overlapping region for all acquisitions per track.\n",
    "\n",
    "6. A similar issue appears when one track **crosses the AOI twice**. In other words some of the frames in the middle of the track are not overlapping the AOI and are already disregarded by step 2. The assembling of the non-subsequent frames during processing would result in a failure. The metadata in the inventory is consequently updated, where the first part of the relative orbit number will be renamed to XXX.1, the second part to XXX.2 and so on. During processing those acquistions will be handled as 2 different tracks, and only merged during the final mosaicking.\n",
    "\n",
    "7. (optional) A last step is needed to assure that for one mosaic in time that consists of different tracks, is only covered once by each track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the search refinement and save all mosaic combinations to \n",
    "# the inventory folder inside your project directory\n",
    "invDict, covDict = refine.searchRefinement(aoi, footprintGdf, invDir)\n",
    "\n",
    "# summing up information\n",
    "for key in invDict:\n",
    "    print('')\n",
    "    print('--------------------------------------------')\n",
    "    print(' Summing up the info about mosaics')\n",
    "    print('--------------------------------------------')\n",
    "    print(' {} mosaics for {}'.format(covDict[key], key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Select best combination and visualize the footprints\n",
    "\n",
    "The above search refinement checks for all possible combinations between orbit direction (i.e. ascending or descending) and polariztaion mode (i.e. single-pol VV, dual-pol VV&VH, single pol HH or dual-pol HH&HV). \n",
    "\n",
    "Outside Europe, usually there is only sufficient data for one orbit direction. Based on the above information, choose the best combination, e.g. DESCENDING_VVVH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the orbit and pol info with most msoaics based\n",
    "mosaicKey = 'DESCENDING_VVVH'\n",
    "\n",
    "# do not edit below\n",
    "if mosaicKey not in invDict.keys():\n",
    "    print(' ERROR: the combination is not avaiable. ' \n",
    "          ' Make sure writing is correct and that sufficient data had been found before.')\n",
    "\n",
    "else: \n",
    "    \n",
    "    pylab.rcParams['figure.figsize'] = (18, 18)\n",
    "    vec.plotInv(aoi, invDict[mosaicKey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. (just execute) Download data\n",
    "\n",
    "Now that we have a refined selection of the scenes we want to process, we can go on and download them. \n",
    "\n",
    "The main entry point is the offcial scihub catalogue from ESA. It is however limited to 2 concurrent donwloads at the same time. \n",
    "\n",
    "A good alternative is the download mirror from the Alaska Satellite Facility, which provides the full archive of Sentinel-1 data. In order ot get registered, go on their data portal at https://vertex.daac.asf.alaska.edu and register. If you already have a NASA Earthdata account, make sure you signed the EULA's needed to access the Copernicus data. A good practice is to try a download directly from the vertex data protal, to assure everything works. \n",
    "\n",
    "By executing the follwing cell, you can select from which data portal you want to dwonload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the OST download routine \n",
    "###we need a zip check routine\n",
    "download.downloadS1(invDict[mosaicKey], dwnDir, concurrent=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
